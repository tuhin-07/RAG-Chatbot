# ğŸ§  RAG Chatbot for Customer Support

A Retrieval-Augmented Generation (RAG) chatbot trained exclusively on customer support documents to assist users with accurate, document-backed responses. If a question falls outside the scope of the provided data, the chatbot will respond with **"I don't know."**

---

## ğŸš€ Features

- âœ… Answers only using provided support documentation.
- âŒ Will not hallucinate or fabricate answers from outside knowledge.
- ğŸ“ Uses FAISS for fast vector-based retrieval.
- ğŸ¤– Integrates with OpenAI (or other LLMs) for contextual response generation.
- ğŸ’¬ User-friendly web interface via Streamlit.
- ğŸ“¦ Self-contained backend with FastAPI.

---

## ğŸ—ï¸ Project Structure

rag-chatbot/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ ingest_docs.py # Embedding and indexing of support documents
â”‚ â”œâ”€â”€ rag_pipeline.py # Retrieval and response generation
â”‚ â”œâ”€â”€ main.py # FastAPI server to handle chat requests
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ app.py # Streamlit web UI for the chatbot
â”œâ”€â”€ data/
â”‚ â””â”€â”€ customer_support_docs/ # Raw support documents (PDF, TXT, etc.)
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # You're reading it!



---

## ğŸ“¦ Requirements

- Python 3.8+
- OpenAI API key (if using OpenAI)
- Recommended: Virtual environment

---

## ğŸ”§ Installation & Setup

### 1. Clone the Repository


```bash
git clone https://github.com/your-username/rag-chatbot.git
cd rag-chatbot

```
### 2. Create a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

```
### 3. Install Dependencies
```bash
pip install -r requirements.txt

```

Step-by-Step Usage
âœ… Step 1: Add Your Documents
Place all your customer support files (.txt, .pdf, etc.) inside:

data/customer_support_docs/

âœ… Step 2: Generate Embeddings & Index
```bash
python backend/ingest_docs.py

This will:

Load and chunk the documents.

Generate embeddings using SentenceTransformer.

Create and save a FAISS index.

Save document chunks in docs.pkl.


```

âœ… Step 3: Run the FastAPI Backend

uvicorn backend.main:app --reload
API will run at: http://127.0.0.1:8000/chat

ğŸ§  How It Works
User sends a query via the frontend.

Query is embedded using the same model as used during ingestion.

FAISS index is searched for top similar document chunks.

If similarity is below threshold, respond: "I don't know."

Else, retrieved context is passed to the LLM (e.g., OpenAI GPT) to generate a grounded response.

ğŸ’¬ Running the Web Interface (Streamlit)
In a new terminal:

```bash
streamlit run frontend/app.py
Visit: http://localhost:8501 to chat with your support bot.


âš™ï¸ Configuration
backend/rag_pipeline.py: Customize the number of top results, similarity threshold, and model used.

.env: (Optional) Store OpenAI API key or other settings.

ğŸ“ Requirements File (requirements.txt)
txt
Copy
Edit
faiss-cpu
sentence-transformers
openai
fastapi
uvicorn
streamlit


ğŸ“¤ Deployment Options
ğŸ”µ Backend: Host on Render, AWS EC2, or Heroku

ğŸŸ¢ Frontend: Deploy via Streamlit Cloud or embed in your site using iframe

ğŸ›¡ï¸ Limitations
Can only answer based on provided documents

Responses depend on the quality and structure of ingested data

"I don't know" logic depends on a similarity thresholdâ€”may require tuning

ğŸ§  Future Enhancements
Add document upload support via web UI

Add feedback/rating system for responses

Explore lightweight open-source LLMs for self-hosting


